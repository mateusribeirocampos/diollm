{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateusribeirocampos/diollm/blob/main/YOLOv3_Dec_seg_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m84GVjNMVIbX",
        "outputId": "2aa84ca1-4004-4f2b-d7b3-97bd71a46419"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação do diretório darknet para o modelo yolo a partir do repositório de Alexey AB."
      ],
      "metadata": {
        "id": "SoUkKWTHHhjR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0obWMEJhTlhv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AlexeyAB/darknet.git\n",
        "%cd darknet\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
        "!make clean && make"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalação da pacote lipopencv-dv e tree\n",
        "\n",
        " It's the development version of OpenCV (Open Source Computer Vision Library), a popular library used for computer vision tasks (like image processing, object detection, etc.). The development version includes header files and libraries needed to build applications that use OpenCV.\n",
        "\n",
        " o pacote tree facilita a visualização da estrutura do projeto"
      ],
      "metadata": {
        "id": "NtTGw8w5HvWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libopencv-dev\n",
        "!sudo apt-get install tree"
      ],
      "metadata": {
        "id": "kE_VhxQAzch7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação dos arquivos\n",
        "\n",
        "Os arquivos vão receber as configurações e os database para o treino, validação e testes."
      ],
      "metadata": {
        "id": "g_EoqEPuYM5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/backup\n",
        "!mkdir -p /content/coco/images"
      ],
      "metadata": {
        "id": "OBf3tNp9V3C3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fazer o download\n",
        "\n",
        "Aqui é feito o download do database todos compactados"
      ],
      "metadata": {
        "id": "-hq2N39XYd3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar val2017 (1GB)\n",
        "!wget -c http://images.cocodataset.org/zips/val2017.zip -P /content/coco/images\n",
        "\n",
        "# Baixar test2017\n",
        "!wget -c http://images.cocodataset.org/zips/test2017.zip -P /content/coco/images\n",
        "\n",
        "# Baixar imagens de treino (18GB)\n",
        "!wget -c http://images.cocodataset.org/zips/train2017.zip -P /content/coco/images"
      ],
      "metadata": {
        "id": "1LU_Obqcb2y0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descompactação do database"
      ],
      "metadata": {
        "id": "zVoAhB3VYtIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/coco/annotations\n",
        "\n",
        "# Baixar anotações (train + val)\n",
        "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P /content/coco/annotations"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CgxsQ6f4id4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/coco/annotations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U54ObAeBkCoi",
        "outputId": "7b8afbd8-46a8-46ad-d522-83174649e885"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotations_trainval2017.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descompactar validações\n",
        "!unzip /content/coco/images/val2017.zip -d /content/coco/images\n",
        "!rm /content/coco/images/val2017.zip\n",
        "\n",
        "# Descompactar imagens de testes\n",
        "!unzip /content/coco/images/test2017.zip -d /content/coco/images\n",
        "!rm /content/coco/images/test2017.zip"
      ],
      "metadata": {
        "id": "PEMYHLIrmTvb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descompactar imagens\n",
        "!unzip /content/coco/images/train2017.zip -d /content/coco/images\n",
        "!rm /content/coco/images/train2017.zip\n",
        "\n",
        "# Descompactar anotações de treino e validações\n",
        "!unzip /content/coco/annotations/annotations_trainval2017.zip -d /content/coco/annotations\n",
        "!rm /content/coco/annotations/annotations_trainval2017.zip"
      ],
      "metadata": {
        "id": "HXB5jQcTaxHu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificação dos arquivos annotations"
      ],
      "metadata": {
        "id": "ZBDkSfkhYy6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/coco/annotations/"
      ],
      "metadata": {
        "id": "1s4GTMRmgCIS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mova todos os JSONs para /content/coco/annotations/\n",
        "!mv /content/coco/annotations/annotations/*.json /content/coco/annotations/\n",
        "!rm -rf /content/coco/annotations/annotations"
      ],
      "metadata": {
        "id": "4OeFes-QmJYX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/coco/annotations/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3w5PV6WmLVy",
        "outputId": "b236e5fa-002d-49f8-e838-438cf446b920"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "captions_train2017.json  instances_train2017.json  person_keypoints_train2017.json\n",
            "captions_val2017.json\t instances_val2017.json    person_keypoints_val2017.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estrutura dos diretórios"
      ],
      "metadata": {
        "id": "yWgsLBAdY412"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tree -d /content/coco/"
      ],
      "metadata": {
        "id": "kb8ZwgnzorK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1874294-aa12-4dcc-8475-e0de1834ff4d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/coco/\u001b[0m\n",
            "├── \u001b[01;34mannotations\u001b[0m\n",
            "└── \u001b[01;34mimages\u001b[0m\n",
            "    ├── \u001b[01;34mtest2017\u001b[0m\n",
            "    ├── \u001b[01;34mtrain2017\u001b[0m\n",
            "    └── \u001b[01;34mval2017\u001b[0m\n",
            "\n",
            "5 directories\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree -d /content/coco/images/"
      ],
      "metadata": {
        "id": "kHTC3e5YuWvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8c6174-c92a-40c2-9a12-8f8170cb5982"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/coco/images/\u001b[0m\n",
            "├── \u001b[01;34mtest2017\u001b[0m\n",
            "├── \u001b[01;34mtrain2017\u001b[0m\n",
            "└── \u001b[01;34mval2017\u001b[0m\n",
            "\n",
            "3 directories\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função de criação dos arquvivos para treino e validação."
      ],
      "metadata": {
        "id": "nbCAdLeiY-pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "def setup_coco_directories(base_dir: str, splits: list):\n",
        "    \"\"\"Cria a estrutura de diretórios para o dataset filtrado\"\"\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    for split in splits:\n",
        "        os.makedirs(os.path.join(base_dir, split, 'images'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(base_dir, split, 'labels'), exist_ok=True)\n",
        "\n",
        "def create_config_files(base_dir: str, categories: list):\n",
        "    \"\"\"Cria os arquivos de configuração obj.names e obj.data\"\"\"\n",
        "    # obj.names\n",
        "    with open(os.path.join(base_dir, 'obj.names'), 'w') as f:\n",
        "        f.write('\\n'.join(categories))\n",
        "\n",
        "    # obj.data (corrigido indentação)\n",
        "    config_content = f\"\"\"classes = {len(categories)}\n",
        "train = {os.path.join(base_dir, 'train.txt')}\n",
        "valid = {os.path.join(base_dir, 'val.txt')}\n",
        "names = {os.path.join(base_dir, 'obj.names')}\n",
        "backup = /content/backup\n",
        "\"\"\"\n",
        "    with open(os.path.join(base_dir, 'obj.data'), 'w') as f:\n",
        "        f.write(config_content)\n",
        "\n",
        "def generate_split_filelists(base_dir: str, splits: list):\n",
        "    \"\"\"Gera os arquivos train.txt e val.txt com caminhos absolutos\"\"\"\n",
        "    for split in splits:\n",
        "        if split == 'test': continue\n",
        "\n",
        "        img_dir = os.path.join(base_dir, split, 'images')\n",
        "        output_file = os.path.join(base_dir, f'{split}.txt')\n",
        "\n",
        "        # Verifica se o diretório de imagens existe\n",
        "        if not os.path.exists(img_dir):\n",
        "            raise FileNotFoundError(f\"Diretório não encontrado: {img_dir}\")\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            for img_name in sorted(os.listdir(img_dir)):\n",
        "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    f.write(f\"{img_dir}/{img_name}\\n\")\n",
        "\n",
        "def process_coco_split(splits: list, categories: list, max_samples_per_split: int = None):\n",
        "    \"\"\"Processa splits do COCO convertendo para formato YOLO\"\"\"\n",
        "    base_dir = '/content/coco/filtered'\n",
        "\n",
        "    # 1. Criar estrutura de diretórios\n",
        "    setup_coco_directories(base_dir, splits)\n",
        "\n",
        "    # 2. Criar arquivos de configuração\n",
        "    create_config_files(base_dir, categories)\n",
        "\n",
        "    # 3. Processar cada split\n",
        "    for split in splits:\n",
        "        print(f\"\\nProcessando split: {split}\")\n",
        "        img_source_dir = f'/content/coco/images/{split}2017'\n",
        "        img_dest_dir = os.path.join(base_dir, split, 'images')\n",
        "\n",
        "        # Verificar origem das imagens\n",
        "        if not os.path.exists(img_source_dir):\n",
        "            raise FileNotFoundError(f\"Diretório de origem não encontrado: {img_source_dir}\")\n",
        "\n",
        "        # Processar imagens\n",
        "        if split == 'test':\n",
        "            images = [{'file_name': f} for f in os.listdir(img_source_dir)]\n",
        "        else:\n",
        "            coco = COCO(f'/content/coco/annotations/instances_{split}2017.json')\n",
        "            cat_ids = coco.getCatIds(catNms=categories)\n",
        "            img_ids = coco.getImgIds(catIds=cat_ids)\n",
        "            images = coco.loadImgs(img_ids[:max_samples_per_split] if max_samples_per_split else img_ids)\n",
        "\n",
        "        # Limitar amostras\n",
        "        if max_samples_per_split and split != 'test':\n",
        "            images = images[:max_samples_per_split]\n",
        "\n",
        "        # Copiar imagens\n",
        "        for img in images:\n",
        "            src = os.path.join(img_source_dir, img['file_name'])\n",
        "            dst = os.path.join(img_dest_dir, img['file_name'])\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "        # Processar anotações para train/val\n",
        "        if split != 'test':\n",
        "            ann_dest_dir = os.path.join(base_dir, split, 'labels')\n",
        "            os.makedirs(ann_dest_dir, exist_ok=True)\n",
        "\n",
        "            coco = COCO(f'/content/coco/annotations/instances_{split}2017.json')\n",
        "\n",
        "            for img in images:\n",
        "                ann_ids = coco.getAnnIds(imgIds=img['id'], catIds=cat_ids)\n",
        "                annotations = coco.loadAnns(ann_ids)\n",
        "\n",
        "                label_path = os.path.join(ann_dest_dir, img['file_name'].replace('.jpg', '.txt'))\n",
        "                with open(label_path, 'w') as f:\n",
        "                    for ann in annotations:\n",
        "                        x, y, w, h = ann['bbox']\n",
        "                        img_w, img_h = img['width'], img['height']\n",
        "\n",
        "                        # Conversão para formato YOLO\n",
        "                        cx = (x + w/2) / img_w\n",
        "                        cy = (y + h/2) / img_h\n",
        "                        nw = w / img_w\n",
        "                        nh = h / img_h\n",
        "\n",
        "                        class_id = categories.index(coco.loadCats(ann['category_id'])[0]['name'])\n",
        "                        f.write(f\"{class_id} {cx:.6f} {cy:.6f} {nw:.6f} {nh:.6f}\\n\")\n",
        "\n",
        "    # 4. Gerar listas de imagens após processamento\n",
        "    generate_split_filelists(base_dir, splits)\n",
        "\n",
        "    # 5. Validar estrutura\n",
        "    print(\"\\nValidação final:\")\n",
        "    !tree {base_dir} -L 2\n",
        "    print(\"\\nExemplo de arquivo train.txt:\")\n",
        "    !head -n 2 {os.path.join(base_dir, 'train.txt')}\n",
        "\n",
        "# Execução\n",
        "process_coco_split(\n",
        "    splits=['train', 'val', 'test'],\n",
        "    categories=['boat', 'person'],\n",
        "    max_samples_per_split=500\n",
        ")"
      ],
      "metadata": {
        "id": "5KkwWlCNdKk_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificação dos diretórios de treino e validação"
      ],
      "metadata": {
        "id": "esURIogweBx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifique se existem e contêm caminhos das imagens\n",
        "!ls /content/coco/filtered/train.txt /content/coco/filtered/val.txt\n",
        "\n",
        "# Exemplo do conteúdo de train.txt (primeiras 2 linhas)\n",
        "!head -n 2 /content/coco/filtered/train.txt"
      ],
      "metadata": {
        "id": "Od-tqcwkx0Gs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tree /content/coco/filtered -L 2"
      ],
      "metadata": {
        "id": "zPF2dxzax4Yg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d42735-0927-4869-9e24-c52260e969f3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/coco/filtered\u001b[0m\n",
            "├── \u001b[00mobj.data\u001b[0m\n",
            "├── \u001b[00mobj.names\u001b[0m\n",
            "├── \u001b[01;34mtest\u001b[0m\n",
            "│   ├── \u001b[01;34mimages\u001b[0m\n",
            "│   └── \u001b[01;34mlabels\u001b[0m\n",
            "├── \u001b[01;34mtrain\u001b[0m\n",
            "│   ├── \u001b[01;34mimages\u001b[0m\n",
            "│   └── \u001b[01;34mlabels\u001b[0m\n",
            "├── \u001b[00mtrain.txt\u001b[0m\n",
            "├── \u001b[01;34mval\u001b[0m\n",
            "│   ├── \u001b[01;34mimages\u001b[0m\n",
            "│   └── \u001b[01;34mlabels\u001b[0m\n",
            "└── \u001b[00mval.txt\u001b[0m\n",
            "\n",
            "9 directories, 4 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificação de Integridade"
      ],
      "metadata": {
        "id": "AZn-Nfw5eIIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dataset(base_dir: str, splits: list):\n",
        "    \"\"\"Valida se número de imagens e labels corresponde\"\"\"\n",
        "    for split in splits:\n",
        "        img_dir = os.path.join(base_dir, split, 'images')\n",
        "        label_dir = os.path.join(base_dir, split, 'labels')\n",
        "\n",
        "        num_images = len(os.listdir(img_dir))\n",
        "        num_labels = len(os.listdir(label_dir)) if split != 'test' else print(\"OK\")\n",
        "\n",
        "        print(f\"{split}: {num_images} imagens, {num_labels} labels\")\n",
        "        if split != 'test' and num_images != num_labels:\n",
        "            raise ValueError(\"Inconsistência entre imagens e labels!\")"
      ],
      "metadata": {
        "id": "Lhr0hXuuYAEb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Controle de Versão dos Arquivos Gerados"
      ],
      "metadata": {
        "id": "D308fR_6ePyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataset_version(base_dir: str):\n",
        "    \"\"\"Salva metadados da versão do dataset\"\"\"\n",
        "    version_info = {\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'categories': ['boat', 'person'],\n",
        "        'splits': ['train', 'val', 'test'],\n",
        "        'num_samples': {split: len(os.listdir(os.path.join(base_dir, split, 'images')))\n",
        "                       for split in ['train', 'val', 'test']}\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(base_dir, 'version.json'), 'w') as f:\n",
        "        json.dump(version_info, f, indent=2)"
      ],
      "metadata": {
        "id": "eyDr4J-5YEuQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Documentação"
      ],
      "metadata": {
        "id": "3TIfN1W0eTt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_documentation(base_dir: str):\n",
        "    \"\"\"Cria arquivo README com informações do dataset\"\"\"\n",
        "    readme_content = f\"\"\"# COCO Filtered Dataset\n",
        "\n",
        "    ## Categories\n",
        "    {', '.join(['boat', 'person'])}\n",
        "\n",
        "    ## Splits\n",
        "    {', '.join(['train', 'val', 'test'])}\n",
        "\n",
        "    ## Statistics\n",
        "    \"\"\"\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        num_images = len(os.listdir(os.path.join(base_dir, split, 'images')))\n",
        "        readme_content += f\"\\n- {split}: {num_images} images\"\n",
        "\n",
        "    with open(os.path.join(base_dir, 'README.md'), 'w') as f:\n",
        "        f.write(readme_content)"
      ],
      "metadata": {
        "id": "XoMftU-6YHUy"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adição dos pesos para treino"
      ],
      "metadata": {
        "id": "ggyyvb7MeXdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "metadata": {
        "id": "WdkVSZXUiGNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704c04e7-bbed-49bb-bbb1-c117fdba3847"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-16 12:00:28--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  16.9MB/s    in 14s     \n",
            "\n",
            "2025-02-16 12:00:43 (16.8 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificação do diretório onde os pesos foram salvos"
      ],
      "metadata": {
        "id": "2NO48XYGeZ4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!which yolov3.weights"
      ],
      "metadata": {
        "id": "C200_GyniY6O"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificação dos nomes"
      ],
      "metadata": {
        "id": "hkaX4yUpehqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/coco/filtered/obj.names"
      ],
      "metadata": {
        "id": "8_zl8w5oxCTq",
        "outputId": "d89c36d8-a976-4cba-d679-def673a80f6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boat\n",
            "person"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificação dos dados"
      ],
      "metadata": {
        "id": "-RxO0BmAemBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/coco/filtered/obj.data"
      ],
      "metadata": {
        "id": "cYBVcPx2xk46",
        "outputId": "bdde2099-b89f-4b9f-e52e-41579c9130f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes = 2\n",
            "train = /content/coco/filtered/train.txt\n",
            "valid = /content/coco/filtered/val.txt\n",
            "names = /content/coco/filtered/obj.names\n",
            "backup = /content/backup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "id": "CrSttZ_j9RHd",
        "outputId": "0ca5461e-a9bf-4044-95f0-c506db73dae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 242M\n",
            "drwxr-xr-x 5 root root 4.0K Feb 16 11:25 3rdparty\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 11:25 backup\n",
            "-rw-r--r-- 1 root root 6.7K Feb 16 12:19 bad.list\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 11:25 build\n",
            "-rwxr-xr-x 1 root root  44K Feb 16 11:25 build.ps1\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 12:01 cfg\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 11:25 cmake\n",
            "-rw-r--r-- 1 root root  30K Feb 16 11:25 CMakeLists.txt\n",
            "-rwxr-xr-x 1 root root 4.8M Feb 16 11:27 darknet\n",
            "-rw-r--r-- 1 root root 1.5K Feb 16 11:25 DarknetConfig.cmake.in\n",
            "-rw-r--r-- 1 root root 9.4K Feb 16 11:25 darknet_images.py\n",
            "-rw-r--r-- 1 root root  11K Feb 16 11:25 darknet.py\n",
            "-rw-r--r-- 1 root root 7.9K Feb 16 11:25 darknet_video.py\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 11:25 data\n",
            "-rw-r--r-- 1 root root  366 Feb 16 11:25 docker-compose.yml\n",
            "-rw-r--r-- 1 root root  774 Feb 16 11:25 Dockerfile.cpu\n",
            "-rw-r--r-- 1 root root  834 Feb 16 11:25 Dockerfile.gpu\n",
            "-rwxr-xr-x 1 root root  110 Feb 16 11:25 image_yolov3.sh\n",
            "-rwxr-xr-x 1 root root  110 Feb 16 11:25 image_yolov4.sh\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 12:28 imgFromWebTest\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 11:25 include\n",
            "-rwxr-xr-x 1 root root  345 Feb 16 11:25 json_mjpeg_streams.sh\n",
            "-rw-r--r-- 1 root root  515 Feb 16 11:25 LICENSE\n",
            "-rw-r--r-- 1 root root 6.5K Feb 16 11:25 Makefile\n",
            "-rwxr-xr-x 1 root root  159 Feb 16 11:25 net_cam_v3.sh\n",
            "-rwxr-xr-x 1 root root  159 Feb 16 11:25 net_cam_v4.sh\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 11:27 obj\n",
            "-rw-r--r-- 1 root root  712 Feb 16 11:25 package.xml\n",
            "-rw-r--r-- 1 root root  67K Feb 16 11:25 README.md\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 11:25 results\n",
            "drwxr-xr-x 4 root root 4.0K Feb 16 11:25 scripts\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 11:25 src\n",
            "-rw-r--r-- 1 root root 2.1K Feb 16 11:25 vcpkg.json\n",
            "-rwxr-xr-x 1 root root  108 Feb 16 11:25 video_yolov3.sh\n",
            "-rwxr-xr-x 1 root root  108 Feb 16 11:25 video_yolov4.sh\n",
            "-rw-r--r-- 1 root root 237M Dec  7  2023 yolov3.weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração dos arquvios para treino"
      ],
      "metadata": {
        "id": "R7o203YSepFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria cópia do cfg original\n",
        "!cp cfg/yolov3.cfg cfg/yolov3_custom.cfg\n",
        "\n",
        "# Aplica modificações\n",
        "!sed -i 's/batch=1/batch=64/' cfg/yolov3_custom.cfg\n",
        "!sed -i 's/subdivisions=1/subdivisions=16/' cfg/yolov3_custom.cfg\n",
        "!sed -i 's/max_batches = 500200/max_batches = 6000/' cfg/yolov3_custom.cfg\n",
        "!sed -i 's/classes=80/classes=2/' cfg/yolov3_custom.cfg\n",
        "!sed -i 's/filters=255/filters=21/' cfg/yolov3_custom.cfg"
      ],
      "metadata": {
        "id": "J6gD6lItiq-L"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name \"yolov3_custom.cfg\" 2>/dev/null"
      ],
      "metadata": {
        "id": "RS5h1LsO9n2z",
        "outputId": "bc8eb83e-59bb-42d5-fbef-561f0daf8fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/darknet/cfg/yolov3_custom.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/darknet/cfg/yolov3_custom.cfg"
      ],
      "metadata": {
        "id": "vOaVX-rT_KAX",
        "outputId": "b7bcb933-b3bb-41c8-b29f-64fb46bade0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[net]\n",
            "# Testing\n",
            "batch=64\n",
            "subdivisions=16\n",
            "# Training\n",
            "# batch=64\n",
            "# subdivisions=166\n",
            "width=416\n",
            "height=416\n",
            "channels=3\n",
            "momentum=0.9\n",
            "decay=0.0005\n",
            "angle=0\n",
            "saturation = 1.5\n",
            "exposure = 1.5\n",
            "hue=.1\n",
            "\n",
            "learning_rate=0.001\n",
            "burn_in=1000\n",
            "max_batches = 6000\n",
            "policy=steps\n",
            "steps=400000,450000\n",
            "scales=.1,.1\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=32\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "# Downsample\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=64\n",
            "size=3\n",
            "stride=2\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=32\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=64\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "# Downsample\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=3\n",
            "stride=2\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=64\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=64\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "# Downsample\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=2\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "# Downsample\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=2\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "# Downsample\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=1024\n",
            "size=3\n",
            "stride=2\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=1024\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=1024\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=1024\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=1024\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[shortcut]\n",
            "from=-3\n",
            "activation=linear\n",
            "\n",
            "######################\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "filters=1024\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "filters=1024\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=512\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "filters=1024\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "filters=21\n",
            "activation=linear\n",
            "\n",
            "\n",
            "[yolo]\n",
            "mask = 6,7,8\n",
            "anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
            "classes=2\n",
            "num=9\n",
            "jitter=.3\n",
            "ignore_thresh = .7\n",
            "truth_thresh = 1\n",
            "random=1\n",
            "\n",
            "\n",
            "[route]\n",
            "layers = -4\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[upsample]\n",
            "stride=2\n",
            "\n",
            "[route]\n",
            "layers = -1, 61\n",
            "\n",
            "\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "filters=512\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "filters=512\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=256\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "filters=512\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "filters=21\n",
            "activation=linear\n",
            "\n",
            "\n",
            "[yolo]\n",
            "mask = 3,4,5\n",
            "anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
            "classes=2\n",
            "num=9\n",
            "jitter=.3\n",
            "ignore_thresh = .7\n",
            "truth_thresh = 1\n",
            "random=1\n",
            "\n",
            "\n",
            "\n",
            "[route]\n",
            "layers = -4\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[upsample]\n",
            "stride=2\n",
            "\n",
            "[route]\n",
            "layers = -1, 36\n",
            "\n",
            "\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "filters=256\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "filters=256\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "filters=128\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "batch_normalize=1\n",
            "size=3\n",
            "stride=1\n",
            "pad=1\n",
            "filters=256\n",
            "activation=leaky\n",
            "\n",
            "[convolutional]\n",
            "size=1\n",
            "stride=1\n",
            "pad=1\n",
            "filters=21\n",
            "activation=linear\n",
            "\n",
            "\n",
            "[yolo]\n",
            "mask = 0,1,2\n",
            "anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
            "classes=2\n",
            "num=9\n",
            "jitter=.3\n",
            "ignore_thresh = .7\n",
            "truth_thresh = 1\n",
            "random=1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execução dos testes"
      ],
      "metadata": {
        "id": "MBvNWFjnet6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./darknet detector train /content/coco/filtered/obj.data /content/darknet/cfg/yolov3_custom.cfg /content/darknet/yolov3.weights -map"
      ],
      "metadata": {
        "id": "cG7Rr9mHwzUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce687df2-8ad1-423a-9543-977f666ee54a"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CUDA-version: 12050 (12040)\n",
            " Warning: CUDA-version is higher than Driver-version! \n",
            ", cuDNN: 9.2.1, GPU count: 1  \n",
            " OpenCV version: 4.5.4\n",
            " Prepare additional network for mAP calculation...\n",
            " 0 : compute_capability = 750, cudnn_half = 0, GPU: Tesla T4 \n",
            "net.optimized_memory = 0 \n",
            "mini_batch = 1, batch = 16, time_steps = 1, train = 0 \n",
            "   layer   filters  size/strd(dil)      input                output\n",
            "   0 Create CUDA-stream - 0 \n",
            " Create cudnn-handle 0 \n",
            "conv     32       3 x 3/ 1    416 x 416 x   3 ->  416 x 416 x  32 0.299 BF\n",
            "   1 conv     64       3 x 3/ 2    416 x 416 x  32 ->  208 x 208 x  64 1.595 BF\n",
            "   2 conv     32       1 x 1/ 1    208 x 208 x  64 ->  208 x 208 x  32 0.177 BF\n",
            "   3 conv     64       3 x 3/ 1    208 x 208 x  32 ->  208 x 208 x  64 1.595 BF\n",
            "   4 Shortcut Layer: 1,  wt = 0, wn = 0, outputs: 208 x 208 x  64 0.003 BF\n",
            "   5 conv    128       3 x 3/ 2    208 x 208 x  64 ->  104 x 104 x 128 1.595 BF\n",
            "   6 conv     64       1 x 1/ 1    104 x 104 x 128 ->  104 x 104 x  64 0.177 BF\n",
            "   7 conv    128       3 x 3/ 1    104 x 104 x  64 ->  104 x 104 x 128 1.595 BF\n",
            "   8 Shortcut Layer: 5,  wt = 0, wn = 0, outputs: 104 x 104 x 128 0.001 BF\n",
            "   9 conv     64       1 x 1/ 1    104 x 104 x 128 ->  104 x 104 x  64 0.177 BF\n",
            "  10 conv    128       3 x 3/ 1    104 x 104 x  64 ->  104 x 104 x 128 1.595 BF\n",
            "  11 Shortcut Layer: 8,  wt = 0, wn = 0, outputs: 104 x 104 x 128 0.001 BF\n",
            "  12 conv    256       3 x 3/ 2    104 x 104 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  13 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  14 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  15 Shortcut Layer: 12,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  16 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  17 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  18 Shortcut Layer: 15,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  19 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  20 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  21 Shortcut Layer: 18,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  22 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  23 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  24 Shortcut Layer: 21,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  25 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  26 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  27 Shortcut Layer: 24,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  28 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  29 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  30 Shortcut Layer: 27,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  31 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  32 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  33 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  34 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  35 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  36 Shortcut Layer: 33,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  37 conv    512       3 x 3/ 2     52 x  52 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  38 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  39 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  40 Shortcut Layer: 37,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  41 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  42 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  43 Shortcut Layer: 40,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  44 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  45 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  46 Shortcut Layer: 43,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  47 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  48 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  49 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  50 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  51 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  52 Shortcut Layer: 49,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  53 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  54 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  55 Shortcut Layer: 52,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  56 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  57 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  58 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  59 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  60 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  61 Shortcut Layer: 58,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  62 conv   1024       3 x 3/ 2     26 x  26 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  63 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  64 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  65 Shortcut Layer: 62,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  66 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  67 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  68 Shortcut Layer: 65,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  69 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  70 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  71 Shortcut Layer: 68,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  72 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  73 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  74 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  75 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  76 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  77 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  78 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  79 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  80 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  81 conv     21       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x  21 0.007 BF\n",
            "  82 yolo\n",
            "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
            "  83 route  79 \t\t                           ->   13 x  13 x 512 \n",
            "  84 conv    256       1 x 1/ 1     13 x  13 x 512 ->   13 x  13 x 256 0.044 BF\n",
            "  85 upsample                 2x    13 x  13 x 256 ->   26 x  26 x 256\n",
            "  86 route  85 61 \t                           ->   26 x  26 x 768 \n",
            "  87 conv    256       1 x 1/ 1     26 x  26 x 768 ->   26 x  26 x 256 0.266 BF\n",
            "  88 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  89 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  90 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  91 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  92 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  93 conv     21       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x  21 0.015 BF\n",
            "  94 yolo\n",
            "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
            "  95 route  91 \t\t                           ->   26 x  26 x 256 \n",
            "  96 conv    128       1 x 1/ 1     26 x  26 x 256 ->   26 x  26 x 128 0.044 BF\n",
            "  97 upsample                 2x    26 x  26 x 128 ->   52 x  52 x 128\n",
            "  98 route  97 36 \t                           ->   52 x  52 x 384 \n",
            "  99 conv    128       1 x 1/ 1     52 x  52 x 384 ->   52 x  52 x 128 0.266 BF\n",
            " 100 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            " 101 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            " 102 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            " 103 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            " 104 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            " 105 conv     21       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x  21 0.029 BF\n",
            " 106 yolo\n",
            "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
            "Total BFLOPS 65.312 \n",
            "avg_outputs = 516922 \n",
            " Allocate additional workspace_size = 52.44 MB \n",
            "yolov3_custom\n",
            " 0 : compute_capability = 750, cudnn_half = 0, GPU: Tesla T4 \n",
            "net.optimized_memory = 0 \n",
            "mini_batch = 4, batch = 64, time_steps = 1, train = 1 \n",
            "   layer   filters  size/strd(dil)      input                output\n",
            "   0 conv     32       3 x 3/ 1    416 x 416 x   3 ->  416 x 416 x  32 0.299 BF\n",
            "   1 conv     64       3 x 3/ 2    416 x 416 x  32 ->  208 x 208 x  64 1.595 BF\n",
            "   2 conv     32       1 x 1/ 1    208 x 208 x  64 ->  208 x 208 x  32 0.177 BF\n",
            "   3 conv     64       3 x 3/ 1    208 x 208 x  32 ->  208 x 208 x  64 1.595 BF\n",
            "   4 Shortcut Layer: 1,  wt = 0, wn = 0, outputs: 208 x 208 x  64 0.003 BF\n",
            "   5 conv    128       3 x 3/ 2    208 x 208 x  64 ->  104 x 104 x 128 1.595 BF\n",
            "   6 conv     64       1 x 1/ 1    104 x 104 x 128 ->  104 x 104 x  64 0.177 BF\n",
            "   7 conv    128       3 x 3/ 1    104 x 104 x  64 ->  104 x 104 x 128 1.595 BF\n",
            "   8 Shortcut Layer: 5,  wt = 0, wn = 0, outputs: 104 x 104 x 128 0.001 BF\n",
            "   9 conv     64       1 x 1/ 1    104 x 104 x 128 ->  104 x 104 x  64 0.177 BF\n",
            "  10 conv    128       3 x 3/ 1    104 x 104 x  64 ->  104 x 104 x 128 1.595 BF\n",
            "  11 Shortcut Layer: 8,  wt = 0, wn = 0, outputs: 104 x 104 x 128 0.001 BF\n",
            "  12 conv    256       3 x 3/ 2    104 x 104 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  13 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  14 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  15 Shortcut Layer: 12,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  16 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  17 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  18 Shortcut Layer: 15,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  19 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  20 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  21 Shortcut Layer: 18,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  22 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  23 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  24 Shortcut Layer: 21,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  25 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  26 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  27 Shortcut Layer: 24,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  28 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  29 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  30 Shortcut Layer: 27,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  31 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  32 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  33 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  34 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  35 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  36 Shortcut Layer: 33,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  37 conv    512       3 x 3/ 2     52 x  52 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  38 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  39 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  40 Shortcut Layer: 37,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  41 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  42 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  43 Shortcut Layer: 40,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  44 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  45 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  46 Shortcut Layer: 43,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  47 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  48 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  49 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  50 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  51 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  52 Shortcut Layer: 49,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  53 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  54 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  55 Shortcut Layer: 52,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  56 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  57 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  58 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  59 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  60 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  61 Shortcut Layer: 58,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  62 conv   1024       3 x 3/ 2     26 x  26 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  63 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  64 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  65 Shortcut Layer: 62,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  66 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  67 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  68 Shortcut Layer: 65,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  69 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  70 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  71 Shortcut Layer: 68,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  72 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  73 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  74 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  75 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  76 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  77 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  78 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  79 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  80 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  81 conv     21       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x  21 0.007 BF\n",
            "  82 yolo\n",
            "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
            "  83 route  79 \t\t                           ->   13 x  13 x 512 \n",
            "  84 conv    256       1 x 1/ 1     13 x  13 x 512 ->   13 x  13 x 256 0.044 BF\n",
            "  85 upsample                 2x    13 x  13 x 256 ->   26 x  26 x 256\n",
            "  86 route  85 61 \t                           ->   26 x  26 x 768 \n",
            "  87 conv    256       1 x 1/ 1     26 x  26 x 768 ->   26 x  26 x 256 0.266 BF\n",
            "  88 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  89 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  90 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  91 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  92 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  93 conv     21       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x  21 0.015 BF\n",
            "  94 yolo\n",
            "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
            "  95 route  91 \t\t                           ->   26 x  26 x 256 \n",
            "  96 conv    128       1 x 1/ 1     26 x  26 x 256 ->   26 x  26 x 128 0.044 BF\n",
            "  97 upsample                 2x    26 x  26 x 128 ->   52 x  52 x 128\n",
            "  98 route  97 36 \t                           ->   52 x  52 x 384 \n",
            "  99 conv    128       1 x 1/ 1     52 x  52 x 384 ->   52 x  52 x 128 0.266 BF\n",
            " 100 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            " 101 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            " 102 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            " 103 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            " 104 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            " 105 conv     21       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x  21 0.029 BF\n",
            " 106 yolo\n",
            "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
            "Total BFLOPS 65.312 \n",
            "avg_outputs = 516922 \n",
            " Allocate additional workspace_size = 149.82 MB \n",
            "Loading weights from /content/darknet/yolov3.weights...\n",
            " seen 64, trained: 32013 K-images (500 Kilo-batches_64) \n",
            "Done! Loaded 107 layers from weights-file \n",
            "saveweights: Using default '1000'\n",
            "savelast: Using default '100'\n",
            "Weights are saved after: 1000 iterations. Last weights (*_last.weight) are stored every 100 iterations. \n",
            "Learning Rate: 0.001, Momentum: 0.9, Decay: 0.0005\n",
            " Detection layer: 82 - type = 28 \n",
            " Detection layer: 94 - type = 28 \n",
            " Detection layer: 106 - type = 28 \n",
            " If error occurs - run training with flag: -dont_show \n",
            "OpenCV exception: draw_train_chart() \n",
            "Saving weights to /content/backup/yolov3_custom_final.weights\n",
            " Create 6 permanent cpu-threads \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000327805.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000073898.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000386119.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000377572.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000263858.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000542181.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000370466.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000573759.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000271987.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000476735.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000344388.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000304999.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000403255.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000328352.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000262509.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000468063.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000223095.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000197886.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000238816.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000018136.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000500861.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000352738.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000205443.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000484321.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000418325.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000009057.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000090476.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000386119.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000450608.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000360767.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000059015.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000336374.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000017667.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000312926.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000197636.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000542181.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000229969.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000411015.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000082576.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000328352.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000254629.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000336675.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000280291.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000271542.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000312926.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000450724.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000018000.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000419056.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000321066.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000058997.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000140020.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000205681.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000499985.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000049602.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000223095.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000017520.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000517570.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000139440.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000287528.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000050518.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000140909.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000074429.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000156869.txt \n",
            "Can't open label file. (This can be normal only if you use MSCOCO): /content/coco/filtered/train/images/000000091912.txt \n",
            "If you want to train from the beginning, then use flag in the end of training command: -clear \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree /content/coco/filtered/train -L 2"
      ],
      "metadata": {
        "id": "Zn_gjiEA0FIN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "metadata": {
        "id": "2nRnmo2S7zro",
        "outputId": "a68e73bc-ffe1-4436-f696-2297780670f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name \"predictions.jpg\" 2>/dev/null"
      ],
      "metadata": {
        "id": "wg-2_az9weug"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p imgFromWebTest\n",
        "!wget -O imgFromWebTest/test_image.jpg https://img.freepik.com/free-photo/back-view-young-citizens-walking-street-with-phones_74855-4948.jpg?t=st=1739710472~exp=1739714072~hmac=592a6dc15dbb5e83e1469b1dadb5ba59c00f6b94c2410a15daffd3cf1b5397bf&w=1060/640/480"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ivKerCQuDfK",
        "outputId": "b30369be-d2a0-4b96-d013-6877c2133087"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-16 14:08:03--  https://img.freepik.com/free-photo/back-view-young-citizens-walking-street-with-phones_74855-4948.jpg?t=st=1739710472~exp=1739714072~hmac=592a6dc15dbb5e83e1469b1dadb5ba59c00f6b94c2410a15daffd3cf1b5397bf\n",
            "Resolving img.freepik.com (img.freepik.com)... 104.109.143.31, 104.109.143.21, 2a02:26f0:1180:19::212:7905, ...\n",
            "Connecting to img.freepik.com (img.freepik.com)|104.109.143.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87372 (85K) [image/jpeg]\n",
            "Saving to: ‘imgFromWebTest/test_image.jpg’\n",
            "\n",
            "\r          imgFromWe   0%[                    ]       0  --.-KB/s               \rimgFromWebTest/test 100%[===================>]  85.32K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-02-16 14:08:03 (1.90 MB/s) - ‘imgFromWebTest/test_image.jpg’ saved [87372/87372]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name \"test_image.jpg\" 2>/dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFd9L55WvG7Z",
        "outputId": "c1439ddf-a6f6-4076-9e1d-f0da0141d322"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/darknet/imgFromWebTest/test_image.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./darknet detector test /content/coco/filtered/obj.data /content/darknet/yolov3.weights /content/backup/yolov3_custom_final.weights /content/darknet/imgFromWebTest/test_image.jpg"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SImmQZjywkAJ",
        "outputId": "ec7e4994-0e13-4b4e-f794-cbd23af3bbc6"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CUDA-version: 12050 (12040)\n",
            " Warning: CUDA-version is higher than Driver-version! \n",
            ", cuDNN: 9.2.1, GPU count: 1  \n",
            " OpenCV version: 4.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/backup/yolov3_custom_final.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VvUYFrZ2Y6o",
        "outputId": "f7531ef4-14ef-432a-fa1b-8c0321c78b6a"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 246326928 Feb 16 12:19 /content/backup/yolov3_custom_final.weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/darknet/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3bGgnlCxBBQ",
        "outputId": "f7fab439-e5a9-4e15-deb4-226101548435"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 242M\n",
            "drwxr-xr-x 5 root root 4.0K Feb 16 11:25 3rdparty\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 11:25 backup\n",
            "-rw-r--r-- 1 root root  20K Feb 16 14:07 bad.list\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 11:25 build\n",
            "-rwxr-xr-x 1 root root  44K Feb 16 11:25 build.ps1\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 12:01 cfg\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 11:25 cmake\n",
            "-rw-r--r-- 1 root root  30K Feb 16 11:25 CMakeLists.txt\n",
            "-rwxr-xr-x 1 root root 4.8M Feb 16 11:27 darknet\n",
            "-rw-r--r-- 1 root root 1.5K Feb 16 11:25 DarknetConfig.cmake.in\n",
            "-rw-r--r-- 1 root root 9.4K Feb 16 11:25 darknet_images.py\n",
            "-rw-r--r-- 1 root root  11K Feb 16 11:25 darknet.py\n",
            "-rw-r--r-- 1 root root 7.9K Feb 16 11:25 darknet_video.py\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 11:25 data\n",
            "-rw-r--r-- 1 root root  366 Feb 16 11:25 docker-compose.yml\n",
            "-rw-r--r-- 1 root root  774 Feb 16 11:25 Dockerfile.cpu\n",
            "-rw-r--r-- 1 root root  834 Feb 16 11:25 Dockerfile.gpu\n",
            "-rwxr-xr-x 1 root root  110 Feb 16 11:25 image_yolov3.sh\n",
            "-rwxr-xr-x 1 root root  110 Feb 16 11:25 image_yolov4.sh\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 12:28 imgFromWebTest\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 11:25 include\n",
            "-rwxr-xr-x 1 root root  345 Feb 16 11:25 json_mjpeg_streams.sh\n",
            "-rw-r--r-- 1 root root  515 Feb 16 11:25 LICENSE\n",
            "-rw-r--r-- 1 root root 6.5K Feb 16 11:25 Makefile\n",
            "-rwxr-xr-x 1 root root  159 Feb 16 11:25 net_cam_v3.sh\n",
            "-rwxr-xr-x 1 root root  159 Feb 16 11:25 net_cam_v4.sh\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 11:27 obj\n",
            "-rw-r--r-- 1 root root  712 Feb 16 11:25 package.xml\n",
            "-rw-r--r-- 1 root root  67K Feb 16 11:25 README.md\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 11:25 results\n",
            "drwxr-xr-x 4 root root 4.0K Feb 16 11:25 scripts\n",
            "drwxr-xr-x 3 root root 4.0K Feb 16 11:25 src\n",
            "-rw-r--r-- 1 root root 2.1K Feb 16 11:25 vcpkg.json\n",
            "-rwxr-xr-x 1 root root  108 Feb 16 11:25 video_yolov3.sh\n",
            "-rwxr-xr-x 1 root root  108 Feb 16 11:25 video_yolov4.sh\n",
            "-rw-r--r-- 1 root root 237M Dec  7  2023 yolov3.weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "image = cv2.imread('predictions.jpg')\n",
        "if image is None:\n",
        "    print(\"Erro: Imagem não encontrada ou corrompida.\")\n",
        "else:\n",
        "    image = cv2.imread('predictions.jpg')\n",
        "    plt.imshow(image)\n",
        "    print(\"Imagem carregada com sucesso.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyLnLc74xM4N",
        "outputId": "bae9c322-246b-4f0d-a962-4433ae32b040"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erro: Imagem não encontrada ou corrompida.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('predictions.jpg')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "h2FvNHuhuK0S",
        "outputId": "33ddfc48-a9b6-42ed-9224-8dcf8c05ade5"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-c04362ac86ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predictions.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    }
  ]
}